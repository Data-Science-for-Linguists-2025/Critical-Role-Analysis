{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5877ce6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  #just in case\n",
    "import nltk\n",
    "from nltk.tokenize import SyllableTokenizer  #for tokenizing syllables\n",
    "from nltk.tokenize import RegexpTokenizer   #for word tokenizing to ignore punctuation\n",
    "import itertools #used later to flatten/merge a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5239da",
   "metadata": {},
   "source": [
    "### How Jargon-y is DND?\n",
    "\n",
    "To take a look, I grabbed my copy of the Player's Handbook and converted it to a text file. I won't be sharing since the copyright belongs to Wizards of the Coast, but I will be discussing my findings here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00196815",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/PHB.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7499b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f348dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "phbtext = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a358d4ff",
   "metadata": {},
   "source": [
    "Our total tokenized word count for the Player's Handbook is below. This excludes punctuation unlike in the usual nltk tokenizing, since the measurements I'm going to be using don't need that information. There's a short glance at a small section to get a feel for what we're looking at. The transformation into a .txt file was not perfect because of some text formatting in the original form, but I did the best I could without spending an excessive amount of time correcting every single issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b62c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(phbtext) #total words (roughly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774ac905",
   "metadata": {},
   "outputs": [],
   "source": [
    "phbtext[8000:8020]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f69aaa",
   "metadata": {},
   "source": [
    "#### On second thought...\n",
    "\n",
    "Some of the contents of this text is stuff like table of contents or an Appendix, which list page numbers. I don't think these would be considered part of the text... or would affect readability of the text. However, this is D&D... There is also a lot of talk about dice and numbers, ability scores, modifiers, rolling a D20, D10, D6 etc. I don't love ditching all instances of numbers, but I'm going to take a look at the different in word count without the page numbers included to see the difference. At worst, I'll run the calculations twice and see how different the scores look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308ad34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonums = re.sub(r\"\\d+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81930d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonumtxt = tokenizer.tokenize(nonums)\n",
    "nonumtxt[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18602f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nonumtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76b9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "phbtext[:10] #for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20618be2",
   "metadata": {},
   "source": [
    "## Flesch–Kincaid readability test\n",
    "<img src=https://wikimedia.org/api/rest_v1/media/math/render/svg/bd4916e193d2f96fa3b74ee258aaa6fe242e110e>\n",
    "\n",
    "This readability test was created to measure how easy a text is to read, it was formulated for use on technical manuals for the military and has been adopted into the educational field as well. The formula above results in a score, the higher the score, the easier the text is to read. There is a scale from 0-100 raning from 5th grade to Professional.\n",
    "\n",
    "[source](https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests)\n",
    "\n",
    "This comes with a translation of the score: The Grade Level Score\n",
    "<img src=https://readable.com/wp-content/uploads/2017/01/fleschkincaidchart.png>\n",
    "\n",
    "[source](https://readable.com/readability/flesch-reading-ease-flesch-kincaid-grade-level/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bda5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSP = SyllableTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a55e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "syls = [SSP.tokenize(w) for w in phbtext]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ff4f1",
   "metadata": {},
   "source": [
    "#### Syllables\n",
    "\n",
    "I tried multiple syllable splitters, and none were perfect, but NLTK's syllable tokenizer was easily the best among them. It's not perfect, and I give it some leeway because we are dealing here with a lot of invented fantasy words, but even regular words it struggles with. See \"appreciative\" and \"rare\" below.\n",
    "\n",
    "It's doing its best, but there's room to grow. We'll take the score we get from our measurement very cautiously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2177c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(syls) #same as tokens - each word is made into a sub-list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b72937",
   "metadata": {},
   "outputs": [],
   "source": [
    "syls[20300:20325]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f14c7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [len(w) for w in syls] #len for each sublist in the larger list of syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fea2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "count[20300:20325] #verification by looking at the same slice as above, looks accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffcf742",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(count)  #sum of all values = count of all syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab100ca7",
   "metadata": {},
   "source": [
    "#### Syllables p.2 (no numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87ce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonumsyls = [SSP.tokenize(w) for w in nonumtxt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2788ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nonumsyls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1f9507",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonumcount = [len(w) for w in nonumsyls]\n",
    "sum(nonumcount) #not a HUGE loss or change, but I'll keep this anyway just to see what changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f132a2f",
   "metadata": {},
   "source": [
    "#### Sentences\n",
    "\n",
    "Again, nltk is the best bet for sentence tokenizing. And, again, we can't be 100% positive that this is entirely accurate. Some of these things are table of contents, some may be sentence fragments, etc. I feel confident this output is more accurate than the syllable tokenizer.\n",
    "\n",
    "Remember, tokenizing sentences on the \"text\" before we removed the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86bb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents[400:405]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a75e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107ef199",
   "metadata": {},
   "source": [
    "#### Sents p.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2cfd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonumsents = nltk.sent_tokenize(nonums)\n",
    "len(nonumsents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c419134",
   "metadata": {},
   "source": [
    "## Running the Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69bbdc2",
   "metadata": {},
   "source": [
    "### First: PHB text including numbers\n",
    "\n",
    "1. Reading Ease\n",
    "2. Grade Level Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20a61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "206.853-(1.015*(219554/11042))-(84.6*(353627/219554))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a3daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.39*(219554/11042))+(11.8*(353627/219554))-15.59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8cc0a4b",
   "metadata": {},
   "source": [
    "### Second: PHB with numbers removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a7f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "206.853-(1.015*(210805/11020))-(84.6*(344878/210805))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd91c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0.39*(210805/11020))+(11.8*(344878/210805))-15.59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa50182",
   "metadata": {},
   "source": [
    "Not all that different! That's interesting, that's cool to see. \n",
    "\n",
    "## Results\n",
    "\n",
    "According to the scores listed, a score of 50 the lowest possible score for the \"Fairly difficult to read\" 10th-12th grade level and the highest for \"difficult to read\" stating a College level. Since the removal of numbers (page numbers but also dice denominations and tutorial text about rolling dice and so on) pushed it just a little more into the \"College\" level difficulty, I think it's reasonable to place the Player's Handbook readability there.\n",
    "\n",
    "The score information at the wiki source for this readability test shares some other scores for popular texts to put this into some perspective:\n",
    "-Time Magazine : 52\n",
    "-Moby Dick : 57.9\n",
    "    - one particularly long sentence about sharks in chapter 64 has a readability score of −146.77\n",
    "-Highest (easiest) possible score is 121.22, every sentence must use only 1 syllable words (think Dr. Seuss!)\n",
    "\n",
    "### Thoughts\n",
    "\n",
    "The Readability Score is interesting. The Player's Handbook is no child's book, necessarily, but is it more difficult than Moby Dick? That gives me pause. It's ultimately a game instruction book and playable for children, is it just down to the fantasy words being used? Prestidigitation, Thaumaturgy, Polymorph... They're tricky words, but once you know the meaning, it's not that crazy.\n",
    "\n",
    "The Grade Score, which is very very similar both for the PHB with and without numbers included, makes sense as a good translation! A score of around 11 putting it on par with Jurassic Park. As someone who has read and owns the PHB, I can agree that feels about right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9260a450",
   "metadata": {},
   "source": [
    "## SMOG Readability Test\n",
    "\n",
    "G. Harry McLaughlin created the SMOG (Simple Measure of Gobbledygook) in 1969 to measure text readability. There is a full breakdown of the formula [here](https://readabilityformulas.com/the-smog-readability-formula/), but it functions similarly to the Flesch–Kincaid test, and I'd like to compare.\n",
    "<img src=https://readabilityformulas.com/wp-content/uploads/01-SMOG-readability-formula.png>\n",
    "\n",
    "The SMOG test is made to be tested on 3 groups of ten sentences, from the beginning, middle, and end of a text, so I'll take some samples rather than the full text. Now, this is likely because the rest was being done by hand at the time of its creation, and this is easier to do than doing the whole book, but these are the instructions and so I will stick to them.\n",
    "\n",
    "So this formula basically can be simplified down to SQ RT of the total number of polysyllabic words plus 3. Since we're sampling only 30 sentences 30/30 is 1 anyway... There are simplified instructions linked below. \n",
    "\n",
    "Link to Ohio State instructions PDF [here](https://ogg.osu.edu/media/documents/health_lit/WRRSMOG_Example.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa670b",
   "metadata": {},
   "source": [
    "### Sampling from the text\n",
    "\n",
    "Knowing the beginning of the book is table of contents and the end is appendix, I want to select for a good option of 30 sentences. I'll do some searching and concatenating into a list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28380652",
   "metadata": {},
   "outputs": [],
   "source": [
    "early = sents[84:94] #some introductory stuff about DND\n",
    "early "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ac6ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "late = sents[9500:9510] #spells and spell descriptions\n",
    "late  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50726245",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = sents[4703:4713] #something around the middle point - looks like information about shopping/money\n",
    "mid  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24750ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "testvals = early+mid+late\n",
    "len(testvals) #30 total sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac315968",
   "metadata": {},
   "source": [
    "### Polysyllabic Words\n",
    "\n",
    "For the SMOG test, these are defined as words 3 syllables or longer\n",
    "\n",
    "wtoks creates a list of lists, which I merge into one long list and comprehend into a list of syllables and then syllable counts. Again, this is a little rough. The syllable counter is..... not the best. You can see in the preview below the mistakes it's making again. This module is good, but it needs a lot of work still. From there, select for only words 3 syllables or longer and get a count of how many appear (not a sum like before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb035935",
   "metadata": {},
   "outputs": [],
   "source": [
    "wtoks = [tokenizer.tokenize(s) for s in testvals] #tokenized sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d31c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = list(itertools.chain(*wtoks))\n",
    "len(merged) #merged into one long list, 651 words in 30 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faaa11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "polysyl = [SSP.tokenize(w) for w in merged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b38038",
   "metadata": {},
   "outputs": [],
   "source": [
    "polysyl[40:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbefa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "polysylval = [len(w) for w in polysyl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "multsyls = [w for w in polysylval if w>=3]\n",
    "multsyls[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a46264",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(multsyls) #89 instances of multisyllabic words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf9fa8",
   "metadata": {},
   "source": [
    "Next step is to find the nearest perfect square to get the sqrt of that value: SQ RT of 81 is **9**. SQ RT of 100 is **10**\n",
    "\n",
    "Adding 3, this gives us grade levels of 12, and 13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a83ad",
   "metadata": {},
   "source": [
    "### One More Thing...\n",
    "\n",
    "With our syllable tokens being a bit unsatisfying to me and with the idea in my mind that this test was designed to be done by hand..... I did it by hand. It's only 30 sentences I'm looking through. Let's see how far off the syllable tokenizer really is. After all, we see that it sometimes splits a 3 syllable word into 2 as well as ther reverse. Maybe it evens out...\n",
    "\n",
    "68 3+ syllable words in our 30 sentences when counted by hand. That's 26 entire instances where a token was incorrectly split into 3 syllables. Not ideal.\n",
    "\n",
    "This means our nearest perfect square is *actually* 64, which takes us to 8 for a grade level score of 11. This puts us quite in agreement with the results of the Flesch–Kincaid test. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ed34b",
   "metadata": {},
   "source": [
    "## Thoughts about Readability Scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031db894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e3b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5753df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSci",
   "language": "python",
   "name": "datasci"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
